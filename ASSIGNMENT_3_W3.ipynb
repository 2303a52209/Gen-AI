{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTtLi9V/xPv1v3098H3PzR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a52209/Gen-AI/blob/main/ASSIGNMENT_3_W3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rtvAuWjNAsz",
        "outputId": "84fa4032-18cb-4155-bcf8-62a4f91e9b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 1 minimum at x = 0.000016, f(x) = 10.000000\n",
            "Function 2 minimum at x = 0.000000, y = 6.229487, g(x,y) = 10.009852\n",
            "Sigmoid function minimum at x = -4.510913, z(x) = 0.010869\n",
            "Linear regression: M = 0.696479, C = 1.851680, Error = 2.510418\n"
          ]
        }
      ],
      "source": [
        "#1\n",
        "def gradient_descent_f1():\n",
        "    def f(x):\n",
        "        return 5 * x**4 + 3 * x**2 + 10\n",
        "\n",
        "    def f_derivative(x):\n",
        "        return 20 * x**3 + 6 * x\n",
        "\n",
        "    x = 2.0\n",
        "    learning_rate = 0.01\n",
        "    max_iterations = 10000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        grad = f_derivative(x)\n",
        "        new_x = x - learning_rate * grad\n",
        "        if abs(new_x - x) < tolerance:\n",
        "            break\n",
        "        x = new_x\n",
        "\n",
        "    return x, f(x)\n",
        "#2\n",
        "def gradient_descent_f2():\n",
        "    from math import exp\n",
        "\n",
        "    def g(x, y):\n",
        "        return 3 * x**2 + 5 * exp(-y) + 10\n",
        "\n",
        "    def g_derivative_x(x, y):\n",
        "        return 6 * x\n",
        "\n",
        "    def g_derivative_y(x, y):\n",
        "        return -5 * exp(-y)\n",
        "\n",
        "    x, y = 2.0, 2.0  # Starting points\n",
        "    learning_rate = 0.01\n",
        "    max_iterations = 10000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        grad_x = g_derivative_x(x, y)\n",
        "        grad_y = g_derivative_y(x, y)\n",
        "\n",
        "        new_x = x - learning_rate * grad_x\n",
        "        new_y = y - learning_rate * grad_y\n",
        "\n",
        "        if abs(new_x - x) < tolerance and abs(new_y - y) < tolerance:\n",
        "            break\n",
        "\n",
        "        x, y = new_x, new_y\n",
        "\n",
        "    return x, y, g(x, y)\n",
        "\n",
        "# 3\n",
        "def gradient_descent_f3():\n",
        "    from math import exp\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(x):\n",
        "        s = sigmoid(x)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    x = 0.0  # Starting point\n",
        "    learning_rate = 0.01\n",
        "    max_iterations = 10000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        grad = sigmoid_derivative(x)\n",
        "        new_x = x - learning_rate * grad\n",
        "        if abs(new_x - x) < tolerance:\n",
        "            break\n",
        "        x = new_x\n",
        "\n",
        "    return x, sigmoid(x)\n",
        "\n",
        "# 4. Finding optimal M and C for linear regression model\n",
        "def gradient_descent_linear_regression(x_data, y_data):\n",
        "    def predicted_output(x, m, c):\n",
        "        return m * x + c\n",
        "\n",
        "    def square_error(m, c):\n",
        "        error = 0\n",
        "        for x, y in zip(x_data, y_data):\n",
        "            pred = predicted_output(x, m, c)\n",
        "            error += (y - pred) ** 2\n",
        "        return error\n",
        "\n",
        "    def derivative_m(m, c):\n",
        "        d_m = 0\n",
        "        for x, y in zip(x_data, y_data):\n",
        "            pred = predicted_output(x, m, c)\n",
        "            d_m += -2 * x * (y - pred)\n",
        "        return d_m\n",
        "\n",
        "    def derivative_c(m, c):\n",
        "        d_c = 0\n",
        "        for x, y in zip(x_data, y_data):\n",
        "            pred = predicted_output(x, m, c)\n",
        "            d_c += -2 * (y - pred)\n",
        "        return d_c\n",
        "\n",
        "    m, c = 0.0, 0.0  # Starting points\n",
        "    learning_rate = 0.0001\n",
        "    max_iterations = 10000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        grad_m = derivative_m(m, c)\n",
        "        grad_c = derivative_c(m, c)\n",
        "\n",
        "        new_m = m - learning_rate * grad_m\n",
        "        new_c = c - learning_rate * grad_c\n",
        "\n",
        "        if abs(new_m - m) < tolerance and abs(new_c - c) < tolerance:\n",
        "            break\n",
        "\n",
        "        m, c = new_m, new_c\n",
        "\n",
        "    return m, c, square_error(m, c)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    x_min, f_min = gradient_descent_f1()\n",
        "    print(f\"Function 1 minimum at x = {x_min:.6f}, f(x) = {f_min:.6f}\")\n",
        "\n",
        "\n",
        "    x_min, y_min, g_min = gradient_descent_f2()\n",
        "    print(f\"Function 2 minimum at x = {x_min:.6f}, y = {y_min:.6f}, g(x,y) = {g_min:.6f}\")\n",
        "\n",
        "    x_min, z_min = gradient_descent_f3()\n",
        "    print(f\"Sigmoid function minimum at x = {x_min:.6f}, z(x) = {z_min:.6f}\")\n",
        "    x_data = [1, 2, 3, 4, 5]\n",
        "    y_data = [2, 4, 5, 4, 5]\n",
        "    m, c, error = gradient_descent_linear_regression(x_data, y_data)\n",
        "    print(f\"Linear regression: M = {m:.6f}, C = {c:.6f}, Error = {error:.6f}\")\n",
        "\n",
        "    #K AKSHAY 2303A52209\n",
        ""
      ]
    }
  ]
}